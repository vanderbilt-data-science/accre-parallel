---
title: "Parallel Programming in R"
author: "Brenton Kenkel"
date: "April 12, 2017"
output:
    md_document:
        variant: markdown_github
---

# Parallel Processing in R

These are the notes for [Brenton Kenkel](http://bkenkel.com)'s workshop in the [Pizza and Programming Seminar](http://www.accre.vanderbilt.edu/?page_id=3243) series at Vanderbilt on April 12, 2017.  These notes assume some familiarity with both the R statistical environment and the SLURM system that ACCRE uses for cluster job management, though I am happy to answer questions about either as they arise.


## Why R?

In theory, R is a general-purpose scripting language like Python or Ruby.  In practice, however, R is used for data analysis.  What sets R apart from the crowd are its out-of-the-box data analysis features:

  * Native support for importing and managing tabular data.

  * Built-in statistical routines, including t-tests, ANOVA, linear regression, generalized linear models.

  * Extensive---and, if used thoughtfully, attractive---data visualization functions.

That's even before we get to the 10,000+ user-contributed packages available through [CRAN](https://cran.r-project.org/web/packages/), the official R package repository.  R's main user base is statisticians, data scientists, academics, and other researchers who work with data.  If you're interested in a new statistical or machine learning technique, there is often a user-friendly R package that implements it.


## Why Parallelize?

Data analysis can be computationally intensive.  You don't want to wait for your results any longer than you have to.

Most of us have access to multiple CPUs, if not through the ACCRE cluster then through multicore processors on our personal computers.  By spreading a computationally intensive task across N CPUs, you can cut your computation time to approximately 1/N of what it would be otherwise.  The trick is to identify which tasks can be distributed, or *parallelized*, in this way.

A task is a good candidate to parallelize if it consists of multiple parts that do not depend on each other's results.  Here are a few examples from [a recent project of mine](http://doe-scores.com) (with Rob Carroll of Florida State) predicting military dispute outcomes that used parallelization extensively:

  * We had ten copies of the data, each with the missing values filled in differently---the result of multiple imputation.  We ran the analysis in parallel across imputations, then averaged the results together at the end.

  * We used our model to generate "predictions" for every pair of countries for every year from 1816 to 2007 (about 1.5 million total).  We split the data up by year, ran 192 separate prediction scripts in parallel, and collected the results together at the end.

  * We assessed the importance of each variable to our model by dropping it from the analysis and re-running.  We ran the 18 drop-one-out analyses in parallel, then collected the results together at the end.

Not all computationally intensive tasks fit the bill.  If step K depends on the results of step K-1, then these steps must be run in sequence.  An example is Markov Chain Monte Carlo---you cannot run a chain in parallel, since the current iteration starts from the previous one.[^mcmc]

[^mcmc]: Though, depending on the nature of your problem, you may be able to use parallelization to reduce computation time for each individual iteration.


## Job Arrays

I will use a minimal data analysis example to illustrate the basic functionality for parallel processing in R.  Time permitting, we will go through a more interesting substantive example at the end of the session.

The file `wdi-data.csv` is country-year data with the following three variables:

  * `female_lfp`: percentage of the country's female ages 15+ in the workforce

  * `fertility`: births per woman

  * `gdppc`: GDP per capita, in constant 2000 USD

Observations range from 1990 to 2014.

```
#exec head -n4 wdi-data.csv
...
#exec tail -n3 wdi-data.csv
```

Suppose we want to examine the relationship between the fertility rate and women's labor force participation across countries in 1990, controlling for each country's overall wealth.  We could use linear regression, via the `lm()` (as in "linear model") function in R.

```{r lm}
wdi_data <- read.csv("wdi-data.csv")

fit_1990 <- lm(female_lfp ~ fertility + log(gdppc),
               data = wdi_data,
               subset = (year == 1990))

coef(fit_1990)
confint(fit_1990)
```

Notice the `response ~ predictor_1 + predictor_2 + ...` syntax, which is called a *formula* in R.

Now suppose we wanted to see how the strength of the relationship varies over time.  Of course, since each regression takes 0.001 seconds, we could easily do that with a standard for loop.  But if we had more data or were using more complex statistics, running each year in sequence might take a long time.

The script `wdi-by-year.r` takes a command line argument specifying the year (0 for 1990, 1 for 1991, etc.), runs the regression for that year, and appends the output to the CSV file `wdi-array-results.csv`:

```r
#include wdi-by-year.r
```

For example, to add the results for the year 2000, we would run:

```sh
Rscript wdi-by-year.r 10
```

What we want to do is run the script for every value from 0 to 24, simultaneously if possible.  We can do this with a SLURM job array, as specified in the SLURM submission script `wdi-by-year.slurm`:

```sh
#include wdi-by-year.slurm
```

When you submit this to ACCRE via `sbatch`, it creates 25 jobs---one for each element of the array.  Depending on your fairshare, bursting limits, and other currently queued jobs, these jobs may run all at once, or a few at a time.


## foreach + MPI

### Packages

To run the code in this section, you will need to have the following packages installed on ACCRE (or whatever machine you are using):

  * **foreach**
  * **doMPI**

To install these, use the command:

```r
install.packages(c("doMC", "foreach"))
```

After installing the packages locally to your user directory, you may need to add the following line to your `.Rprofile` to ensure that R can find them:

```r
.libPaths(c(.libPaths(),
            paste0("~/R/library/", as.character(getRversion()))))
```

### Syntax

If we weren't thinking about parallelization, the natural way to run our analysis for each year from 1990 to 2014 would be with a for loop.

```{r use-for}
years <- 1990:2014
output <- matrix(NA, nrow = length(years), ncol = 4)
colnames(output) <- c("year", "ci_low", "estimate", "ci_high")

for (i in 1:length(years)) {
    fit <- lm(female_lfp ~ fertility + log(gdppc),
              data = wdi_data,
              subset = (year == years[i]))

    output[i, "year"] <- years[i]
    output[i, "ci_low"] <- confint(fit)["fertility", 1]
    output[i, "estimate"] <- coef(fit)["fertility"]
    output[i, "ci_high"] <- confint(fit)["fertility", 2]
}

output
```

Notice that the i'th step of the loop doesn't depend on the results of the i-1'th step, so this loop is a candidate for parallelization.  The easiest way to parallelize the loop is to follow these steps:

1.  Rewrite the loop using the `foreach()` function provided by the **foreach** package.

2.  Register a "parallel backend" for `foreach()` through one of the "do" packages (**doMPI**, **doMC**, **doSNOW**, etc.).

Both of these steps are fairly easy.  The syntax for `foreach()` is similar to that of a for loop, with two differences.  First, `foreach()` is a function that returns a list, each of whose elements is the value calculated in the corresponding iteration of the loop.  Therefore, unlike with for loops, there is no need to set up storage for the output in advance.  Second, because of this, there cannot be interdependencies between steps of a `foreach()` loop.

`wdi-mpi.r` is a script that reimplements the loop above with `foreach()`, using MPI to parallelize:

```r
#include wdi-mpi.r
```

The syntax in our SLURM submission script also changes when we use MPI:

```sh
#include wdi-mpi.slurm
```

This script requests six nodes for computation.  One of those nodes will be used to run the main script; when it reaches the `foreach()` loop, it will distribute tasks across the other five.

### Pros and Cons

Explicit parallelization via MPI (or another backend) has some pros and cons relative to the job array approach.

  * **Cleaner code.**  With a job array, you need a separate script to collect your results and perform further analysis.  With explicit parallelization, everything can be in one place.

  * **Requires less storage.**  With a job array, you must save your intermediate results to a hard disk in order for the collection script to access them.  This is burdensome if the intermediate results are large.

    By the same token, though, explicit parallelization may require access to more memory during the computation itself.

  * **Less flexible.**  If you write a script to use MPI, it won't run on your local machine (unless you've set up MPI on your personal laptop).  You can fix this by placing the cluster setup inside an if-else condition, at the expense of the "cleaner code" ideal.

  * **Less robust.**  If you run an array of 100 jobs and 3 of them fail, you only need to re-run those 3.  But if some of the jobs fail within a `foreach()` loop, unless you have been exceedingly careful in your coding, you will have to re-run the whole thing.

  * **More fairshare usage.**  While the `foreach()` loop is running, the node running the main script is mostly idly waiting for results, eating up your fairshare.  And as the loop reaches its end, the nodes that have finished also sit idle while waiting for the stragglers.


## A Less Trivial Example

For this example, we'll work with `adult-cleaned.csv`, a cleaned-up version of the "Adult" data hosted at [the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Adult).  This is census data, with the goal being to predict whether the respondent earns more or less than $50,000/year.

```{r adult}
adult_data <- read.csv("adult-cleaned.csv")
head(adult_data)
```

We will use k-nearest neighbors, a simple but powerful predictive algorithm.  An important problem is to choose the "tuning parameter" k, the number of nearest neighbors to use to make the prediction for each observation.  A common approach is to choose k using 10-fold cross-validation.  This is computationally intensive---it entails fitting the model 10 times for each candidate value of k.

The **caret** package has a function `train()` for user-friendly tuning and training of machine-learning models.  A great feature of `train()` is that it uses `foreach()` and automatically detects whether we have registered a parallel backend, so as to parallelize the cross-validation process when possible.

The script `adult-mpi.r` compares the k-nearest neighbors training process with and without the benefit of parallelization.

```r
#include adult-mpi.r
```

And we have the associated SLURM submission script:

```sh
#include adult-mpi.slurm
```


## Footnotes
